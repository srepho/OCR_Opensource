{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 02 - Tier 1 Benchmark (T4 GPU)\n\nRun all Tier 1 models on the stratified sample set.\nDesigned for Colab Pro with T4 GPU.\n\n## Setup\n1. Upload `colab_data.zip` to your Google Drive root\n2. Select **T4 GPU** runtime in Colab\n3. Run all cells in order\n\n**Runtime**: ~2 hours for all Tier 1 models on 100 pages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Colab Setup: clone repo, install deps, unpack data ===\n!git clone https://github.com/srepho/OCR_Opensource.git 2>/dev/null || echo \"Already cloned\"\n%cd OCR_Opensource\n!pip install -q -e .\n!pip install -q -r requirements/colab_tier1.txt\n\n# Mount Google Drive and unpack data\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport zipfile, os\nDATA_ZIP = '/content/drive/MyDrive/colab_data.zip'\nif os.path.exists(DATA_ZIP):\n    with zipfile.ZipFile(DATA_ZIP, 'r') as zf:\n        zf.extractall('.')\n    print(f\"Unpacked data from {DATA_ZIP}\")\nelse:\n    print(f\"WARNING: {DATA_ZIP} not found - upload colab_data.zip to Drive root\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch\nfrom pathlib import Path\n\nPROJECT_ROOT = Path(\".\").resolve()\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\nif torch.cuda.is_available():\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n\n# Verify data is present\nassert Path(\"data/sample_sets/stratified_100.json\").exists(), \"Sample sets missing - check data setup\"\nassert Path(\"data/images\").exists(), \"Images missing - check data setup\"\nprint(\"Data check OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\nwith open(\"config/benchmark_config.yaml\") as f:\n    config = yaml.safe_load(f)\n\nSAMPLE_SET = \"data/sample_sets/stratified_100.json\"\nIMAGE_DIR = config[\"paths\"][\"image_dir\"]\nOUTPUT_DIR = config[\"paths\"][\"raw_outputs_dir\"]\n\n# Tier 1 models (T4-compatible, <=8GB VRAM)\nTIER1_MODELS = [\n    \"doctr\",\n    \"lighton_ocr\",\n    \"got_ocr2\",\n    \"florence2\",\n    \"dots_ocr\",\n    \"deepseek_ocr\",\n    \"nanonets_ocr\",\n    \"ocrflux\",\n    \"granite_vision\",\n    \"monkey_ocr\",\n    \"paddleocr_vl15\",\n    \"qwen25_vl_3b\",\n    \"granite_docling_258m\",\n]\n\nprint(f\"Will run {len(TIER1_MODELS)} models on {SAMPLE_SET}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Quick Validation (1 page per model)\n\nRun each model on a single page to identify failures before committing to the full benchmark. Review the results below â€” models that PASS can be included in the full run.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import gc, json, time, traceback\nfrom PIL import Image\nfrom src.pipeline.runner import instantiate_adapter, load_model_registry\n\nregistry = load_model_registry(\"config/model_registry.yaml\")\n\n# Load one test image\nsample = json.loads(Path(\"data/sample_sets/quick_dev.json\").read_text())\ntest_page = sample[\"pages\"][0]\ntest_image_path = Path(IMAGE_DIR) / test_page[\"pdf_stem\"] / f\"page_{test_page['page_num']:03d}.png\"\ntest_image = Image.open(str(test_image_path)).convert(\"RGB\")\nprint(f\"Test image: {test_image_path.name} ({test_image.size})\\n\")\n\nvalidation_results = {}\n\nfor model_key in TIER1_MODELS:\n    print(f\"--- {model_key} ---\")\n    adapter = None\n    try:\n        # Phase 1: instantiate\n        adapter = instantiate_adapter(model_key, registry, device=\"cuda\")\n        \n        # Phase 2: load model weights\n        t0 = time.time()\n        adapter.load_model()\n        load_time = time.time() - t0\n        \n        # Phase 3: run inference on 1 page\n        t0 = time.time()\n        result = adapter.ocr_page(test_image)\n        infer_time = time.time() - t0\n        \n        text_preview = result.text[:150].replace('\\n', ' ')\n        print(f\"  PASS  load={load_time:.1f}s  infer={infer_time:.1f}s  chars={len(result.text)}  preview: {text_preview}\")\n        validation_results[model_key] = \"PASS\"\n        \n    except Exception as e:\n        print(f\"  FAIL  {type(e).__name__}: {e}\")\n        traceback.print_exc(limit=2)\n        validation_results[model_key] = f\"FAIL: {e}\"\n    finally:\n        if adapter is not None:\n            try:\n                adapter.unload_model()\n            except Exception:\n                pass\n            del adapter\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    print()\n\n# Summary\nprint(\"=\" * 60)\nprint(\"VALIDATION SUMMARY\")\nprint(\"=\" * 60)\npassed = [k for k, v in validation_results.items() if v == \"PASS\"]\nfailed = [k for k, v in validation_results.items() if v != \"PASS\"]\nfor k, v in validation_results.items():\n    status = \"PASS\" if v == \"PASS\" else \"FAIL\"\n    print(f\"  [{status}] {k}\")\nprint(f\"\\n{len(passed)}/{len(validation_results)} models passed\")\n\n# Update TIER1_MODELS to only include passing models for full run\nTIER1_VALIDATED = passed\nprint(f\"\\nModels for full benchmark: {TIER1_VALIDATED}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Full Benchmark (validated models only)\n\nRuns all validated models on the full stratified_100 sample set. Only models that passed the single-page validation above are included.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.pipeline.runner import run_model_on_sample_set\n\nprofiles = {}\n\nfor model_key in TIER1_VALIDATED:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {model_key}\")\n    print(f\"{'='*60}\")\n    try:\n        profile = run_model_on_sample_set(\n            model_key=model_key,\n            sample_set_path=SAMPLE_SET,\n            image_dir=IMAGE_DIR,\n            output_dir=OUTPUT_DIR,\n            registry=registry,\n            device=\"cuda\",\n            skip_existing=True,\n        )\n        profiles[model_key] = profile\n    except Exception as e:\n        print(f\"FAILED: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    # Clear GPU memory between models\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        import gc; gc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary of all runs\nimport pandas as pd\n\nrows = []\nfor model_key, profile in profiles.items():\n    rows.append({\n        \"model\": model_key,\n        \"pages\": profile.total_pages,\n        \"avg_sec/page\": round(profile.avg_time_per_page, 2),\n        \"pages/min\": round(profile.pages_per_minute, 1),\n        \"peak_GPU_MB\": round(profile.peak_gpu_memory_mb, 0),\n    })\n\ndf = pd.DataFrame(rows)\nprint(df.to_string(index=False))\n\n# Save results back to Drive for persistence\nimport shutil\nDRIVE_RESULTS = '/content/drive/MyDrive/ocr_results'\nshutil.copytree('results/raw_outputs', f'{DRIVE_RESULTS}/raw_outputs', dirs_exist_ok=True)\nprint(f\"\\nResults saved to {DRIVE_RESULTS}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}