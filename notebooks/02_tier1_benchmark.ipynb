{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Tier 1 Benchmark (T4 GPU)\n",
    "\n",
    "Run all Tier 1 models on the stratified sample set.\n",
    "Designed for Colab Pro with T4 GPU.\n",
    "\n",
    "**Runtime**: ~2 hours for all Tier 1 models on 100 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "# !pip install -q -r requirements/colab_tier1.txt\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(PROJECT_ROOT / \"config\" / \"benchmark_config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "SAMPLE_SET = PROJECT_ROOT / \"data\" / \"sample_sets\" / \"stratified_100.json\"\n",
    "IMAGE_DIR = PROJECT_ROOT / config[\"paths\"][\"image_dir\"]\n",
    "OUTPUT_DIR = PROJECT_ROOT / config[\"paths\"][\"raw_outputs_dir\"]\n",
    "\n",
    "# Tier 1 models in recommended order\n",
    "TIER1_MODELS = [\n",
    "    \"doctr\",\n",
    "    \"lighton_ocr\",\n",
    "    \"got_ocr2\",\n",
    "    \"florence2\",\n",
    "    \"dots_ocr\",\n",
    "    \"deepseek_ocr\",\n",
    "    \"nanonets_ocr\",\n",
    "    \"ocrflux\",\n",
    "    \"granite_vision\",\n",
    "    \"monkey_ocr\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.runner import run_model_on_sample_set, load_model_registry\n",
    "\n",
    "registry = load_model_registry(str(PROJECT_ROOT / \"config\" / \"model_registry.yaml\"))\n",
    "profiles = {}\n",
    "\n",
    "for model_key in TIER1_MODELS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    try:\n",
    "        profile = run_model_on_sample_set(\n",
    "            model_key=model_key,\n",
    "            sample_set_path=SAMPLE_SET,\n",
    "            image_dir=IMAGE_DIR,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            registry=registry,\n",
    "            device=\"cuda\",\n",
    "            skip_existing=True,\n",
    "        )\n",
    "        profiles[model_key] = profile\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all runs\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for model_key, profile in profiles.items():\n",
    "    rows.append({\n",
    "        \"model\": model_key,\n",
    "        \"pages\": profile.total_pages,\n",
    "        \"avg_sec/page\": round(profile.avg_time_per_page, 2),\n",
    "        \"pages/min\": round(profile.pages_per_minute, 1),\n",
    "        \"peak_GPU_MB\": round(profile.peak_gpu_memory_mb, 0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
