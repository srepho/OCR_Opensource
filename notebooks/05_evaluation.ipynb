{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Evaluation\n",
    "\n",
    "Compute all metrics for each model's outputs against ground truth.\n",
    "Run locally after all benchmark notebooks complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import yaml\n",
    "with open(PROJECT_ROOT / \"config\" / \"benchmark_config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "with open(PROJECT_ROOT / \"config\" / \"model_registry.yaml\") as f:\n",
    "    registry = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SET = PROJECT_ROOT / \"data\" / \"sample_sets\" / \"stratified_100.json\"\n",
    "RAW_OUTPUTS = PROJECT_ROOT / config[\"paths\"][\"raw_outputs_dir\"]\n",
    "GT_TEXT = PROJECT_ROOT / config[\"paths\"][\"embedded_text_dir\"]\n",
    "GT_TABLES = PROJECT_ROOT / config[\"paths\"][\"table_gt_dir\"]\n",
    "METRICS_DIR = PROJECT_ROOT / config[\"paths\"][\"metrics_dir\"]\n",
    "\n",
    "# Find which models have outputs\n",
    "available_models = [d.name for d in RAW_OUTPUTS.iterdir() if d.is_dir()]\n",
    "print(f\"Models with outputs: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.evaluator import evaluate_all_models\n",
    "\n",
    "results = evaluate_all_models(\n",
    "    model_keys=available_models,\n",
    "    sample_set_path=SAMPLE_SET,\n",
    "    raw_outputs_dir=RAW_OUTPUTS,\n",
    "    gt_text_dir=GT_TEXT,\n",
    "    gt_tables_dir=GT_TABLES if GT_TABLES.exists() else None,\n",
    "    metrics_dir=METRICS_DIR,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for model_key, result in results.items():\n",
    "    agg = result.get(\"aggregate\", {})\n",
    "    text_agg = agg.get(\"text\", {})\n",
    "    row = {\"model\": model_key, \"pages\": result.get(\"total_pages_evaluated\", 0)}\n",
    "    for metric in [\"ned\", \"cer\", \"wer\", \"bleu\", \"fuzzy_ratio\"]:\n",
    "        if metric in text_agg:\n",
    "            row[metric] = text_agg[metric][\"mean\"]\n",
    "    if \"ned\" in row:\n",
    "        row[\"text_accuracy\"] = round((1 - row[\"ned\"]) * 100, 2)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"text_accuracy\", ascending=False)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per content-type breakdown\n",
    "for model_key in available_models[:3]:  # Top 3\n",
    "    result = results[model_key]\n",
    "    pages = result.get(\"per_page\", [])\n",
    "    by_type = {}\n",
    "    for p in pages:\n",
    "        ct = p.get(\"content_type\", \"unknown\")\n",
    "        by_type.setdefault(ct, []).append(p[\"text_metrics\"][\"ned\"])\n",
    "    \n",
    "    print(f\"\\n{model_key}:\")\n",
    "    for ct, neds in sorted(by_type.items()):\n",
    "        import statistics\n",
    "        mean_ned = statistics.mean(neds)\n",
    "        print(f\"  {ct}: NED={mean_ned:.4f}, accuracy={100*(1-mean_ned):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
