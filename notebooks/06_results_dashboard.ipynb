{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Results Dashboard\n",
    "\n",
    "Generate comparison visualizations across all models.\n",
    "Run locally after `05_evaluation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import yaml\n",
    "with open(PROJECT_ROOT / \"config\" / \"benchmark_config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import load_all_results, generate_summary_table\n",
    "\n",
    "METRICS_DIR = PROJECT_ROOT / config[\"paths\"][\"metrics_dir\"]\n",
    "OUTPUTS_DIR = PROJECT_ROOT / config[\"paths\"][\"raw_outputs_dir\"]\n",
    "DASHBOARD_DIR = PROJECT_ROOT / \"results\" / \"dashboard\"\n",
    "\n",
    "df = load_all_results(METRICS_DIR, OUTPUTS_DIR)\n",
    "print(f\"Loaded {len(df)} page evaluations across {df['model'].nunique()} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary = generate_summary_table(df)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import plot_accuracy_bar\n",
    "\n",
    "fig = plot_accuracy_bar(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import plot_accuracy_vs_speed\n",
    "\n",
    "fig = plot_accuracy_vs_speed(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import plot_radar\n",
    "\n",
    "fig = plot_radar(df, top_n=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import plot_content_type_heatmap\n",
    "\n",
    "fig = plot_content_type_heatmap(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import plot_metric_distributions\n",
    "\n",
    "fig = plot_metric_distributions(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reporting.dashboard import generate_full_dashboard\n",
    "\n",
    "generate_full_dashboard(METRICS_DIR, OUTPUTS_DIR, DASHBOARD_DIR)\n",
    "print(f\"\\nDashboard saved to {DASHBOARD_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative comparison (side-by-side viewer)\n",
    "from src.reporting.qualitative import generate_comparison_report\n",
    "\n",
    "SAMPLE_SET = PROJECT_ROOT / \"data\" / \"sample_sets\" / \"quick_dev.json\"\n",
    "GT_TEXT = PROJECT_ROOT / config[\"paths\"][\"embedded_text_dir\"]\n",
    "IMAGE_DIR = PROJECT_ROOT / config[\"paths\"][\"image_dir\"]\n",
    "\n",
    "# Get available models\n",
    "model_keys = [d.name for d in Path(OUTPUTS_DIR).iterdir() if d.is_dir()]\n",
    "\n",
    "generate_comparison_report(\n",
    "    sample_set_path=SAMPLE_SET,\n",
    "    model_keys=model_keys[:4],  # Limit to 4 models for readability\n",
    "    raw_outputs_dir=OUTPUTS_DIR,\n",
    "    gt_text_dir=GT_TEXT,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    output_path=DASHBOARD_DIR / \"qualitative_comparison.html\",\n",
    "    max_pages=20,\n",
    ")\n",
    "print(\"Qualitative comparison saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
