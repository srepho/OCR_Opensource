{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 - Traditional Baselines (CPU)\n\nRun traditional OCR baselines: PaddleOCR, EasyOCR, DocTR, Tesseract.\nThese models run on CPU. Can run locally or on Colab.\n\n## Setup (Colab)\n1. Upload `colab_data.zip` to your Google Drive root\n2. Any runtime is fine (CPU sufficient)\n3. Run all cells in order"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Setup: clone repo, install deps, unpack data ===\n# Uncomment for Colab:\n# !git clone https://github.com/srepho/OCR_Opensource.git 2>/dev/null || echo \"Already cloned\"\n# %cd OCR_Opensource\n# !pip install -q -e \".[traditional]\"\n# !apt-get install -q -y tesseract-ocr\n# from google.colab import drive\n# drive.mount('/content/drive')\n# import zipfile, os\n# DATA_ZIP = '/content/drive/MyDrive/colab_data.zip'\n# if os.path.exists(DATA_ZIP):\n#     with zipfile.ZipFile(DATA_ZIP, 'r') as zf:\n#         zf.extractall('.')\n\n# For local: pip install -e \".[traditional]\" && brew install tesseract"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\nPROJECT_ROOT = Path(\".\").resolve()\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nimport yaml\nwith open(\"config/benchmark_config.yaml\") as f:\n    config = yaml.safe_load(f)\n\nSAMPLE_SET = \"data/sample_sets/stratified_100.json\"\nIMAGE_DIR = config[\"paths\"][\"image_dir\"]\nOUTPUT_DIR = config[\"paths\"][\"raw_outputs_dir\"]\n\nCPU_MODELS = [\"tesseract\", \"paddleocr\", \"easyocr\", \"doctr\"]\n\nassert Path(\"data/sample_sets/stratified_100.json\").exists(), \"Sample sets missing\"\nprint(f\"Will run {len(CPU_MODELS)} models on {SAMPLE_SET}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Quick Validation (1 page per model)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import gc, json, time, traceback\nfrom PIL import Image\nfrom src.pipeline.runner import instantiate_adapter, load_model_registry\n\nregistry = load_model_registry(\"config/model_registry.yaml\")\n\n# Load one test image\nsample = json.loads(Path(\"data/sample_sets/quick_dev.json\").read_text())\ntest_page = sample[\"pages\"][0]\ntest_image_path = Path(IMAGE_DIR) / test_page[\"pdf_stem\"] / f\"page_{test_page['page_num']:03d}.png\"\ntest_image = Image.open(str(test_image_path)).convert(\"RGB\")\nprint(f\"Test image: {test_image_path.name} ({test_image.size})\\n\")\n\nvalidation_results = {}\n\nfor model_key in CPU_MODELS:\n    print(f\"--- {model_key} ---\")\n    adapter = None\n    try:\n        adapter = instantiate_adapter(model_key, registry, device=\"cpu\")\n        t0 = time.time()\n        adapter.load_model()\n        load_time = time.time() - t0\n        \n        t0 = time.time()\n        result = adapter.ocr_page(test_image)\n        infer_time = time.time() - t0\n        \n        text_preview = result.text[:150].replace('\\n', ' ')\n        print(f\"  PASS  load={load_time:.1f}s  infer={infer_time:.1f}s  chars={len(result.text)}  preview: {text_preview}\")\n        validation_results[model_key] = \"PASS\"\n    except Exception as e:\n        print(f\"  FAIL  {type(e).__name__}: {e}\")\n        traceback.print_exc(limit=2)\n        validation_results[model_key] = f\"FAIL: {e}\"\n    finally:\n        if adapter is not None:\n            try:\n                adapter.unload_model()\n            except Exception:\n                pass\n            del adapter\n        gc.collect()\n    print()\n\nprint(\"=\" * 60)\nprint(\"VALIDATION SUMMARY\")\nprint(\"=\" * 60)\npassed = [k for k, v in validation_results.items() if v == \"PASS\"]\nfor k, v in validation_results.items():\n    status = \"PASS\" if v == \"PASS\" else \"FAIL\"\n    print(f\"  [{status}] {k}\")\nprint(f\"\\n{len(passed)}/{len(validation_results)} models passed\")\n\nCPU_VALIDATED = passed\nprint(f\"\\nModels for full benchmark: {CPU_VALIDATED}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Full Benchmark (validated models only)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.pipeline.runner import run_model_on_sample_set\n\nprofiles = {}\n\nfor model_key in CPU_VALIDATED:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {model_key}\")\n    print(f\"{'='*60}\")\n    try:\n        profile = run_model_on_sample_set(\n            model_key=model_key,\n            sample_set_path=SAMPLE_SET,\n            image_dir=IMAGE_DIR,\n            output_dir=OUTPUT_DIR,\n            registry=registry,\n            device=\"cpu\",\n            skip_existing=True,\n        )\n        profiles[model_key] = profile\n    except Exception as e:\n        print(f\"FAILED: {e}\")\n        import traceback\n        traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\nrows = []\nfor model_key, profile in profiles.items():\n    rows.append({\n        \"model\": model_key,\n        \"pages\": profile.total_pages,\n        \"avg_sec/page\": round(profile.avg_time_per_page, 2),\n        \"pages/min\": round(profile.pages_per_minute, 1),\n    })\n\ndf = pd.DataFrame(rows)\nprint(df.to_string(index=False))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}