{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 - Tier 2 Benchmark (A100 GPU)\n\nRun Tier 2 models (7B parameters) on the stratified sample set.\nRequires Colab Pro with A100 GPU.\n\n## Setup\n1. Upload `colab_data.zip` to your Google Drive root\n2. Select **A100 GPU** runtime in Colab\n3. Run all cells in order\n\n**Runtime**: ~1 hour for all Tier 2 models on 100 pages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Colab Setup: clone repo, install deps, unpack data ===\n!git clone https://github.com/srepho/OCR_Opensource.git 2>/dev/null || echo \"Already cloned\"\n%cd OCR_Opensource\n!pip install -q -e .\n!pip install -q -r requirements/colab_tier2.txt\n\n# Mount Google Drive and unpack data\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport zipfile, os\nDATA_ZIP = '/content/drive/MyDrive/colab_data.zip'\nif os.path.exists(DATA_ZIP):\n    with zipfile.ZipFile(DATA_ZIP, 'r') as zf:\n        zf.extractall('.')\n    print(f\"Unpacked data from {DATA_ZIP}\")\nelse:\n    print(f\"WARNING: {DATA_ZIP} not found - upload colab_data.zip to Drive root\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch\nfrom pathlib import Path\n\nPROJECT_ROOT = Path(\".\").resolve()\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\nif torch.cuda.is_available():\n    vram_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n    print(f\"VRAM: {vram_gb:.1f} GB\")\n    assert vram_gb >= 16, f\"Need >= 16GB VRAM for Tier 2, have {vram_gb:.1f}GB\"\n\nassert Path(\"data/sample_sets/stratified_100.json\").exists(), \"Sample sets missing\"\nprint(\"Data check OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\nwith open(\"config/benchmark_config.yaml\") as f:\n    config = yaml.safe_load(f)\n\nSAMPLE_SET = \"data/sample_sets/stratified_100.json\"\nIMAGE_DIR = config[\"paths\"][\"image_dir\"]\nOUTPUT_DIR = config[\"paths\"][\"raw_outputs_dir\"]\n\nTIER2_MODELS = [\n    \"olmocr\",\n    \"rolmocr\",\n    \"qwen25_vl\",\n    \"chandra\",\n]\n\nprint(f\"Will run {len(TIER2_MODELS)} models on {SAMPLE_SET}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.pipeline.runner import run_model_on_sample_set, load_model_registry\n\nregistry = load_model_registry(\"config/model_registry.yaml\")\nprofiles = {}\n\nfor model_key in TIER2_MODELS:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {model_key}\")\n    print(f\"{'='*60}\")\n    try:\n        profile = run_model_on_sample_set(\n            model_key=model_key,\n            sample_set_path=SAMPLE_SET,\n            image_dir=IMAGE_DIR,\n            output_dir=OUTPUT_DIR,\n            registry=registry,\n            device=\"cuda\",\n            skip_existing=True,\n        )\n        profiles[model_key] = profile\n    except Exception as e:\n        print(f\"FAILED: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    # Clear GPU memory between models\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        import gc; gc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary of all runs\nimport pandas as pd\n\nrows = []\nfor model_key, profile in profiles.items():\n    rows.append({\n        \"model\": model_key,\n        \"pages\": profile.total_pages,\n        \"avg_sec/page\": round(profile.avg_time_per_page, 2),\n        \"pages/min\": round(profile.pages_per_minute, 1),\n        \"peak_GPU_MB\": round(profile.peak_gpu_memory_mb, 0),\n    })\n\ndf = pd.DataFrame(rows)\nprint(df.to_string(index=False))\n\n# Save results back to Drive\nimport shutil\nDRIVE_RESULTS = '/content/drive/MyDrive/ocr_results'\nshutil.copytree('results/raw_outputs', f'{DRIVE_RESULTS}/raw_outputs', dirs_exist_ok=True)\nprint(f\"\\nResults saved to {DRIVE_RESULTS}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}