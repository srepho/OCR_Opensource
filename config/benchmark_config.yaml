# Master benchmark configuration

paths:
  project_root: "."
  pdf_dir: "data/pdfs"
  image_dir: "data/images"
  ground_truth_dir: "data/ground_truth"
  embedded_text_dir: "data/ground_truth/embedded_text"
  table_gt_dir: "data/ground_truth/tables"
  sample_sets_dir: "data/sample_sets"
  raw_outputs_dir: "results/raw_outputs"
  metrics_dir: "results/metrics"
  comparison_dir: "results/comparison"

# PDF zip batches (relative to project root or absolute)
zip_batches:
  - "pds-batch-1.zip"
  - "pds-batch-2.zip"
  - "pds-batch-3.zip"
  - "pds-batch-4.zip"
  - "pds-batch-5.zip"

rendering:
  dpi: 200
  format: "png"
  max_pages_per_pdf: null  # null = all pages

ground_truth:
  primary_extractor: "pdfplumber"
  cross_check_extractor: "pymupdf"
  discrepancy_threshold: 0.05  # flag if NED > 5% between extractors
  char_ratio_threshold: 0.85   # min(len_a,len_b)/max — above this, assume reading-order difference → prefer PyMuPDF
  extra_text_threshold: 1.15   # if pdfplumber has >15% more text, prefer pdfplumber

sampling:
  random_seed: 42
  sample_sets:
    quick_dev:
      pages: 20
      description: "Adapter development and debugging"
    stratified_100:
      pages: 100
      docs: 20
      pages_per_doc: 5
      description: "Primary benchmark set"
    table_focus:
      pages: 50
      description: "Pages with tables for TEDS evaluation"
    full_benchmark:
      pages: 300
      description: "Comprehensive comparison set"

evaluation:
  text_metrics:
    - ned          # Normalized Edit Distance (rapidfuzz)
    - cer          # Character Error Rate (jiwer)
    - wer          # Word Error Rate (jiwer)
    - bleu         # BLEU score (sacrebleu)
    - fuzzy_ratio  # Token sort ratio (rapidfuzz)
  table_metrics:
    - teds         # Tree Edit Distance based Similarity
  composite_weights:
    text_accuracy: 0.333   # (1-NED)*100
    table_accuracy: 0.333  # TEDS
    layout_score: 0.333    # Layout similarity
  ranking:
    primary_metrics:
      - text_accuracy
      - table_accuracy
      - coverage_rate
    # Weighted score for leaderboard ordering:
    # rank_score = ((w_text * text_accuracy) + (w_table * table_accuracy_or_text)) * coverage_rate
    weights:
      text_accuracy: 0.7
      table_accuracy: 0.3
  uncertainty:
    bootstrap_resamples: 1000
    confidence_level: 0.95
    random_seed: 42

inference_protocol:
  deterministic: true
  global_seed: 42
  canonical_instruction: "Extract all text from this document image. Preserve layout, tables, and formatting as markdown."
  decoding:
    do_sample: false
    temperature: 0.0
    top_p: 1.0
    max_new_tokens: 4096

robustness_slices:
  clean:
    enabled: true
  rotate_2deg:
    enabled: true
    rotate_degrees: 2.0
  gaussian_blur:
    enabled: true
    gaussian_blur_radius: 1.2
  jpeg_75:
    enabled: true
    jpeg_quality: 75
  downscale_75:
    enabled: true
    downscale_factor: 0.75

normalization:
  unicode: "NFKC"
  strip_markdown: true
  collapse_whitespace: true
  remove_page_numbers: true
  lowercase: false  # preserve case for evaluation
